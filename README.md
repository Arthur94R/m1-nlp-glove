# üé¨ TP2 ‚Äî Construction embeddings avec GloVe & comparaison avec Word2Vec (TP1)

Projet universitaire ‚Äî Master 1 IA & Big Data, Universit√© Paris 8

## üìã Description

Comparaison de deux m√©thodes d'embeddings sur le dataset de films :
- **Word2Vec** : Approche pr√©dictive (fen√™tre glissante, Skip-gram)
- **GloVe** : Approche statistique (matrice de co-occurrence globale)

L'objectif est d'analyser les diff√©rences entre ces deux techniques et de visualiser comment elles capturent le sens s√©mantique des mots.

## üéØ Objectifs

- Entra√Æner Word2Vec et GloVe sur le m√™me corpus
- Comparer les mots similaires trouv√©s par chaque m√©thode
- Visualiser les embeddings avec t-SNE
- Mesurer la corr√©lation entre les deux espaces vectoriels

## üîç Diff√©rences cl√©s

| Aspect | Word2Vec | GloVe |
|--------|----------|-------|
| **Approche** | Pr√©dictive (locale) | Statistique (globale) |
| **Principe** | Pr√©dit le contexte √† partir d'un mot | Factorisation de la matrice de co-occurrence |
| **Focus** | Fen√™tre glissante (contexte imm√©diat) | Statistiques globales du corpus entier |
| **M√©thode** | R√©seau de neurones (Skip-gram) | Optimisation : `vecteur1 ¬∑ vecteur2 ‚âà log(co_occ)` |

## üõ†Ô∏è Stack technique

- **Python 3.13** ‚Äî Langage principal
- **Gensim** ‚Äî Word2Vec
- **TensorFlow** ‚Äî Entra√Ænement GloVe
- **NLTK** ‚Äî Preprocessing
- **Scikit-learn** ‚Äî t-SNE
- **Pandas / NumPy** ‚Äî Traitement des donn√©es
- **Matplotlib** ‚Äî Visualisations

## üìÅ Structure

```
TP2_Word2Vec_vs_GloVe/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ movies_metadata.csv              ‚Üí Dataset films (√† t√©l√©charger)
‚îÇ   ‚îú‚îÄ‚îÄ word2vec_films.bin               ‚Üí Mod√®le W2V entra√Æn√©
‚îÇ   ‚îú‚îÄ‚îÄ glove_embeddings_films.npy       ‚Üí Vecteurs GloVe
‚îÇ   ‚îî‚îÄ‚îÄ glove_vocab_films.npy            ‚Üí Vocabulaire GloVe
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                          ‚Üí Word2Vec (TP1)
‚îÇ   ‚îú‚îÄ‚îÄ glove_films.py                   ‚Üí Entra√Ænement GloVe
‚îÇ   ‚îî‚îÄ‚îÄ compare_embeddings.py            ‚Üí Comparaison W2V vs GloVe
‚îî‚îÄ‚îÄ results/
    ‚îú‚îÄ‚îÄ glove_tsne.png                   ‚Üí Visualisation GloVe
    ‚îú‚îÄ‚îÄ w2v_vs_glove_comparison.png      ‚Üí Comparaison t-SNE
    ‚îî‚îÄ‚îÄ similarity_correlation.png       ‚Üí Corr√©lation des similarit√©s
```

## üì• Donn√©es

**Dataset √† t√©l√©charger :**
- `movies_metadata.csv` ‚Üí [Kaggle - The Movies Dataset](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset)

Placer dans le dossier `data/`.

## üöÄ Installation et lancement

### Installation
```bash
# Installer les d√©pendances
pip install pandas numpy matplotlib scikit-learn gensim nltk tensorflow

# T√©l√©charger ressources NLTK
python -c "import nltk; nltk.download('stopwords'); nltk.download('punkt')"
```

### Lancement

**√âtape 1 : GloVe**
```bash
python src/glove_films.py
# Dur√©e : ~5-10 minutes
# G√©n√®re : glove_embeddings_films.npy, glove_vocab_films.npy
```

**√âtape 2 : Comparaison**
```bash
python src/compare_embeddings.py
# G√©n√®re les graphiques de comparaison
```

## üìä R√©sultats attendus

### 1. Mots similaires

**Exemple pour "love" :**

```
Word2Vec :
  romance         : 0.735
  affection       : 0.741
  madly           : 0.730

GloVe :
  romance         : 0.820
  passion         : 0.798
  heart           : 0.765
```

**Observation :** Certains voisins sont communs, d'autres diff√®rent.

### 2. Visualisation t-SNE

Deux graphiques c√¥te √† c√¥te montrant l'organisation des mots dans chaque espace vectoriel.

**Diff√©rences possibles :**
- Clusters diff√©rents
- Distances relatives modifi√©es
- Certains mots mieux s√©par√©s dans un mod√®le

### 3. Corr√©lation

```
Corr√©lation W2V vs GloVe : 0.72
```

**Interpr√©tation :**
- **r > 0.7** ‚Üí Mod√®les assez corr√©l√©s (capturent des infos similaires)
- **r ~ 0.5** ‚Üí Diff√©rences notables
- **r < 0.3** ‚Üí Tr√®s diff√©rents

## üéì Concepts cl√©s

### Word2Vec (Skip-gram)
- **Principe** : Pr√©dit les mots du contexte √† partir d'un mot central
- **Apprentissage** : R√©seau de neurones avec negative sampling
- **Avantage** : Rapide, capture bien le contexte local

### GloVe (Global Vectors)
- **Principe** : Factorise la matrice de co-occurrence globale
- **Objectif** : `vecteur(mot1) ¬∑ vecteur(mot2) ‚âà log(co_occurrence)`
- **Avantage** : Capture les statistiques globales du corpus

### Matrice de co-occurrence
Compte combien de fois deux mots apparaissent ensemble dans une fen√™tre :
```
"I love romantic comedy films"

Co-occurrence (window=5) :
  (love, romantic) : 1.0
  (love, comedy)   : 0.5
  (love, I)        : 1.0
  ...
```

### t-SNE (t-Distributed Stochastic Neighbor Embedding)
R√©duit les dimensions (100D ‚Üí 2D) en pr√©servant les distances relatives pour visualisation.

## üìà Analyse comparative

### Points communs
- Les deux capturent la similarit√© s√©mantique
- Mots similaires ont des vecteurs proches
- Corr√©lation g√©n√©ralement > 0.6

### Diff√©rences
- **Word2Vec** : Meilleur sur le contexte imm√©diat et syntaxe
- **GloVe** : Meilleur sur les relations s√©mantiques globales et analogies
- **W2V** : Plus rapide √† entra√Æner
- **GloVe** : Plus stable (d√©terministe)

## üìù Livrables

- ‚úÖ Code source (Word2Vec, GloVe, comparaison)
- ‚úÖ Embeddings entra√Æn√©s
- ‚úÖ Visualisations comparatives
- ‚úÖ Analyse des corr√©lations
- ‚úÖ README

## üîó Lien avec TP1

Ce TP2 √©tend le TP1 en :
- Ajoutant une deuxi√®me m√©thode d'embeddings (GloVe)
- Comparant syst√©matiquement les r√©sultats
- Analysant les forces/faiblesses de chaque approche

## üìö R√©f√©rences

- **Word2Vec** : Mikolov et al. (2013) - "Efficient Estimation of Word Representations in Vector Space"
- **GloVe** : Pennington et al. (2014) - "GloVe: Global Vectors for Word Representation"
- **t-SNE** : van der Maaten & Hinton (2008) - "Visualizing Data using t-SNE"

## üí° Observations typiques

**Corr√©lation √©lev√©e (r > 0.7) :**
- Les deux mod√®les capturent des informations similaires
- Diff√©rences subtiles dans l'organisation de l'espace

**Corr√©lation moyenne (r ~ 0.5) :**
- Approches compl√©mentaires
- GloVe capture mieux certaines relations globales
- W2V capture mieux le contexte local

**Cas d'usage :**
- **Word2Vec** : Classification de texte, analyse de sentiment
- **GloVe** : Analogies, relations s√©mantiques complexes